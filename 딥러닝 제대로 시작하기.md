BOOK : 딥러닝 제대로 시작하기
===========

## *※ STEP 1*

</br >

## *※ STEP 2*

* Q. 기본적인 신경망 계산 과정을 간단하게 설명해주세요
    * 훈련 샘플을 입력하는 feed-forward(앞먹임) 계산을 하고 오차를 구한 뒤, 이어 역전파 계산을 하고 오차의 기울기를 구하여 가중치를 업데이트합니다.
</br >

## *※ STEP 3*
* Q. **과적합**이란 무엇인가?
    * A. 과적합이란, 학습 시에 오차함수의 값이 작은 극소점(local minimum)에 갇힌 상황이다.
* Q. 과적합을 **완화**시키는 방법에는 무엇이 있죠?
    * A. 과적합을 완화시키려면 규제화(regularization)를 해야하는데 방법으론 가중치 감쇠(weight decay), 가중치 상한,  드롭아웃(drop-out) 등이 있어요.
    * Q-1. 왜 위와 같은 방법들이 제안되었어요?
        * 신경망의 자유도를 낮추는 것은, 과다한 자유도를 가질 경우를 제외하고는 그다지 바람직하지 않아요. 그래서 학습 시에 가중치의 자유도를 제약하는 규제화에 의해 과적합 문제를 완화하도록 몇 가지 방법이 제안됐어요.
    * Q-2. **가중치 감쇠(weight decay)** 는 무엇이죠?
        * 가장 간단한 규제화 방법은 가중치에 어떤 제약을 가하는 거에요. 그중에서도 오차함수에 **가중치의 제곱합(norm의 제곱)** 을 더한 뒤, 이를 최소화하는 방법이 가중치 감쇠에요.
        * 추가로, **람다(λ)** 는 이 규제화의 강도를 제어하는 파라미터인데요, 이 람다를 추가하여 가중치는 자신의 크기에 비례하는 속도로 항상 감쇠하도록 업데이트돼요.
    * Q-3. **가중치 상한**은요?
        * 각 유닛의 입력 측 가중치에 대해서 그 제곱합의 최댓값을 제약하는 방법이에요.
        * 만약에 가중치 상한 부등식을 만족하지 않는다면, 가중치에 미리 정한 (1보다 작은) 상수를 곱하여 부등식을 만족하도록 만들어요.
    * Q-4. 마지막으로 **드롭아웃(drop-out)** 이 무엇이죠?
        * 다층(multi-layer) 신경망의 유닛 중 일부를 확률적으로 선택하여 학습하는 방법이에요.
        * 자세히 말하면, 중간층과 입력층 각 층의 유닛 중 미리 정해 둔 비율 p만큼을 선택하고 선택되지 않은 유닛을 무효화 취급합니다.
        * 즉, 가중치를 업데이트할 때마다 다시 무작위로 선택되며 학습이 끝나고 추론 시에는 모든 유닛을 사용하여 feed-forward(앞먹임) 계산을 합니다.
        * 드롭아웃의 대상이 되었던 층의 유닛은 모든 출력을 p배로 합니다.(또는 출력의 가중치를 p배로 해요.)
            * 그 이유는 추론시의 유닛 수가 학습 시에 비해 1/p배 된 것과 같기 때문에 이를 보상하기 위함이에요.
        * 특히, 클래스 분류의 출력층에 사용되는 소프트맥스 층에 대해서는 드롭아웃 방법이 출력의 기하평균을 내는 것과 같음을 알 수 있어요.
    * Q-5. RBM이나 오토인코더의 학습에 드롭아웃을 적용하면 어떤 효과를 얻을 수 있죠?
        * 두 경우 모두 희소적 특징이 자동 학습된다는 결과가 보고되었어요. 즉, 희소 규제화를 거치지 않고도 유사한 특징을 얻을 수 있다는 이야기죠.
    * Q-6. 드롭아웃과 비슷한 방법이 무엇이 있죠?
        * 드롭 커넥트와 확률적 최대 풀링이 있어요.
* Q. **데이터 정규화(normalization of data)** 가 뭐에요?
    * 각 샘플 Xn을 **선형 변환**하는건데 (Xn-X평균)/표준편차로 구할 수 있어요.
    * 데이터 정규화를 거친 샘플은 각 성분의 평균이 0, 분산이 1이 돼요.
    * 특정한 성분의 분산이 0이거나 매우 작은 경우가 있으면 max(표준편차,충분히 작은값)으로 나누어줘요.
* Q. **데이터의 확장(data augmentation)** 이란 무엇이죠?
    * 데이터 확장은요 샘플 데이터를 일정하게 가공해서 양적으로 '물타기'하는 작업이라고 해요.
    * Q-1. 이걸 왜 하는거죠?
        * 확장을 하는 이유는 트레이닝 데이터의 부족이 **과적합**을 일으키는 가장 큰 원인이기 때문이에요.
    * 이미지 데이터같이 샘플의 분포 양상을 예상할 수 있는 경우에 특히 유효해요. 예를 들어서 같은 카테고리의 물체 이미지이기만 하면 어느 정도 변화를 가해도 타당한 훈련 샘플이라고 보기 때문이에요.
    * 가우스 분포를 따르는 **랜덤 노이즈**를 일괄적으로 적용하는 방법도 유효한 방법이에요. 
 * Q. **학습률(Learning Rate)** 이 무엇이며 그것이 왜 중요한가요?
    * 경사 하강법에서는 **파라미터의 업데이트 정도**를 학습률을 통해 조절해요. 학습률 설정은 학습의 성패를 좌우할 정도로 중요해요.
    * Q-1. 학습률은 어떻게 결정하나요?
        * 학습률을 결정하는데에 정석과 같은 두 가지 방법이 있어요.
        * 첫 번째는, 학습 초기에 값을 크게 설정했다가 학습을 하면서 학습률을 점점 줄여가는 방법이에요.
        * 두 번째는, 신경망의 모든 층에서 같은 학습률을 사용하는 것이 아니라 층마다 서로 다른 값을 사용하는 방법이에요.
            * 예를 들어, 출력 방향에 가까운 얕은 층에서는 학습률을 작게 잡고, 입력에 가까운 깊은 층에는 크게 잡는 경우가 있어요.
            * 또한, CNN처럼 **가중치 공유**를 할 때 학습률을 가중치가 공유되는 수의 제곱근에 비례하도록 설정하면, 가중치의 업데이트 속도를 대체로 비슷하게 맞출 수 있어요.
* Q. 위의 질문과는 달리 학습률을 자동으로 결정하는 방법에는 뭐가 있죠?
    * AdaGrad가 있어요. 이거는 직관적으로, **자주 나타나는** 기울기의 성분보다 **드물게 나타나는** 기울기 성분을 더 중시해서 파라미터를 업데이트하는 방법이에요.
* Q. **모멘텀(momentum)** 을 설명해주세요.
    * 모멘텀은 경사 하강법의 **수렴 성능을 향상시키기 위한 방법** 중 하나인데요, 가중치의 업데이트 값에 이전 업데이트 값의 일정 비율을 더하는 방법이에요.
    * Q-1. 모멘텀의 효과는 무엇이죠?
        * 오차함수가 깊은 골짜기 같은 형상을 가지는데, 골짜기 바닥이 평평할 때 약간만 빠져나와도 골짜기와 직교하는 방향으로 큰 기울기가 생기면서 바닥을 정상적으로 탐색하지 못하는 문제가 있어요
        * 이러한 문제를 해결하고 골짜기 방향을 따라 골짜기 바닥을 효율적으로 탐색할 수 있게 만들어주죠.
* Q. **가중치의 초기화** 하는 방법을 말해주세요.
    * 가장 일반적인것은 가우스 분포로부터 랜덤값을 생성하여 초기값으로 삼는 방법이에요. 그리고 가우스 분포의 표준편차 선택도 중요한데, 보통 표준편차를 크게 잡으면 초기 학습은 빠르게 진행되지만, 오차함수의 감소가 일찌감치 멈춰버리는 경향이 있을 수 있어요.

</br >

## *※ STEP 4 : 역전파 * [책보기]
* Q. **역전파법(backpropagation)** 이 뭔가요?
    * 역전파는 앞먹임 신경망 학습에서 가중치와 바이어스에 대한 오차함수의 미분을 계산해야하는데 이러한 미분을 효율적으로 계산하는 방법이에요.
    * Q-1. 역전파법을 왜 사용하죠?
        * **경사 하강법**을 실행하기 위해서는 오차함수 E(w)의 기울기를 계산해야 하는데, 이 미분의 계산이 매우 까다롭기 때문에 역전파법을 사용하는거죠.
        * 각 층의 결합 가중치(w)와 각 유닛의 바이어스(b)에 대한 오차함수의 편미분이 기울기 벡터의 각 성분이고,
        자세히는 중간층, 특히 입력이 가까운 깊은 층의 파라미터일수록 미분을 계산하기 까다로워요.
* Q. 오차 역전파를 통해 **오차 기울기(가중치에 대한 오차의 미분)를 계산하는 절차** 를 말해줘요.
    * 1. 각각의 층의 유닛 입력 u과 출력 z을 순서대로 계산한다.
    * 2. 출력층 델타(δ)를 구한다. (통상적으로 δ = z - d : 출력층 L의 유닛 j의 델타 δ는 신경망의 출력(z)과 목표 출력(d)의 차가 된다.)
    * 3. **역전파** : 각 중간층 l( = L-1, L-2, .., 2)에서의 델타 δ를 출력층부터 가까운 순서대로 계산한다.
    * 4. 각 층 l(= 2, ..., L))의 파라미터 w에 대해 미분을 계산한다.
    * **참조** : l-1번째 층의 유닛 i와 l번째 층의 유닛 j를 잇는 결합의 가중치 w_ji에 대한 미분은,
    유닛 j에 대한 **델타(δ_j)(L)** 와 유닛 i의 **출력 z_i(L-1)** 의 **곱**에 지나지 않는다.
* Q. 순전파와 역전파 계산의 **공통점과 차이점**은?
    * **공통점** : 순전파와 역전파 계산은 모두 층 단위의 행렬 계산으로 나타낼 수 있으며 식의 형태가 닮았다는 공통점이 있다.
    * **차이점** : 순전파는 **비선형 계산**인데 비해, 역전파는 **선형 계산**이라는 차이점이 있다.
        * 순전파 계산에서는 각 층에 대한 입력은 유닛이 갖는 활성화 함수를 경유하기 때문에, 활성화 함수가 비선형이라면 이 층의 입출력의 관계도 비선형성을 갖는다.
            * ex) 로지스틱 함수를 예로 들면 각 층의 출력은 항상 [0, 1]의 범위로 제약되며, 값이 지나치게 커져서 발산해 버리는 일은 일어나지 않는다.
        * 한편, 역전파 계산은 선형 계산이다. 그 결과, 각 층의 가중치의 값이 크면 델타가 각 층을 거쳐 전달되는 도중에 **급속하게 커지거나(발산)**, 혹은 반대로 기울기가 작으면 **급속하게 작아져 0(소실)** 이 되어 버린다. 어떤 경우든 가중치의 업데잍트가 잘안되며 학습 자체가 어려워진다.

</br>

## *※ STEP 5 : 자기부호화기(autoencoder)*
* Q. 오토인코더의 목적이 뭐에요?
    * 오토인코더는 **목표 출력없이 입력만으로** 구성된 트레이닝 데이터로 **비지도 학습**을 수행하여 데이터의 특징을 나타내는, 더 나은 표현을 얻는 것이 목표인 신경망입니다.
    * 또한, 딥 네트워크의 사전훈련(pre-training), 즉 그 **가중치의 좋은 초기값**을 얻는 목적으로 이용됩니다.
    * 자세히 설명하면, feature의 학습을 통해 샘플 x의 또 '다른' 표현인 y를 얻는 것이고, 직관적으로 x를 그대로 쓰는 대신 변환된 y를 사용하는 것입니다.
* Q. 오토인코더의 학습 목표가 뭐죠?
    * 오토인코더의 학습의 목표는 입력을 **부호화(encode)** 한 뒤, 이어 다시 **복호화(decode)** 했을 때 원래의 입력을 되도록 충실히 재현할 수 있는 부호화 방법을 찾는 것이에요.
    * 또한, 오토인코더는 **오차함수를 최소화하는 과정**을 통해 신경망의 가중치와 바이어스를 결정하게 됩니다.
* Q. 오토인코더의 **활성화 함수**와 **오차함수**는 보통 무엇을 사용하죠?
    * 오토인코더의 활성화 함수중에서 중간층의 f는 자유롭게 바꿀수 있으며 통상적으로 **비선형함수**를 사용합니다. 그리고 출력층의 f'은 신경망의 목표 출력이 입력한 x 자신이 될 수 있도록 입력 데이터의 유형에 따라 선택합니다.
    * 1.x의 성분이 실수값을 가질때, 출력층의 f'은 항등사상으로 하는 것이 좋고 오차함수로는 입력값과 출력값에 차에 대한 제곱합을 사용합니다.
    * 2.x의 성분이 이진값을 갖는 경우에는 f'으로 로지스틱 함수를 보통 사용하고 오차함수로는 교차 엔트로피를 사용합니다.
* Q. 오토인코더를 결정하는 요소에는 뭐가 있을까요?
    * 주로 중간층의 유닛 수와 해당층에서 사용되는 활성화 함수가 있죠.
* Q. 오토인코더의 최적화에 대해서 주의할 점을 간단히 설명해주세요.
    * 오토인코더의 최적화는 확률적 경사 하강법의 샘플 추출 시를 제외하면, 랜덤성 없이 결정론적으로 진행된다는 점에 주의하면 됩니다.
* Q. **과완비(overcomplete)** 란 무엇이죠?
    * 오토인코더는 입력 데이터의 **feature(자질)** 를 학습함으로써, 입력 데이터에서 불필요한 정보를 제거하고 그 본질만을 추출하는 겁니다.
    * 이러한 이유 때문에 입력 데이터의 성분 수 Dx보다도 인코딩된 부호가 갖는 성분 수 Dy는 자연히 더 작을 거라고 생각하는데 항상 그렇지만은 않아요.
    * 즉, 희소 규제화를 이용한다면 여분의 자유도를 갖는 특징이어도 입력 데이터를 잘 나타내는 자질을 얻는 것이 가능해지는데, 이것을 **과완비(overcomplete)** 한 표현이라고 합니다.
* Q. 그럼 **희소 오토인코더(sparse autoencoder)** 는요?
    * 예를 들어서 중간층에 활성화 함수로 선형함수를 사용한 경우, 중간층의 유닛수 Dy가 입력층의 유닛 수 Dx보다 많다면 무의미한 결과밖에 얻을 수 없습니다. 활성화 함수에 비선형함수를 사용하면 이러한 논의는 처음부터 성립할 수 없지만, 중간층의 자유도가 입력의 자유도보다 크다는 것은 변하지 않으며, 쓸모없는 해만을 얻게 될 가능성이 높습니다. 이에 대해 희소 규제화를 사용하면, 증간층의 유닛 수가 더 많은 경우(Dy >= Dx)에도 오토인코더가 의미 있는 표현을 학습할 수 있게 되는데 이를 **희소 오토인코더**라고 부릅니다.

* Q. **희소규제화**에 대해서 좀 더 자세히 설명해주세요.
    * 기본적인 아이디어는 훈련 샘플 Xn을 되도록 적은 수의 중간층 유닛을 사용하여 재현할 수 있도록 파라미터를 결정하는 것입니다.
    * 또한, 입력 x로부터 중간층의 출력 y를 거쳐 출력 x'가 계산되는 과정에 있어서, y의 각 유닛 중 되도록 적은 수의 유닛만이 0이 아닌 출력치를 갖고 나머지는 출력이 0이 되도록(=발화하지 않음)하는 제약을 가합니다.
        * 원래의 오차함수 E(w)에 희소 규제화 항을 추가한 E'(w)를 최소화한다.
    * 간단히는 활성도의 평균값이 작아지도록 제약을 가하고 각 샘픔을 표현하는 데 쓰이는 중간층 유닛의 수가 적어지도록 학습을 진행한다고 설명할 수 있습니다.
* Q. 희소규제화에서 **로(ρ)** 와 **베타(β)** 는 무엇을 의미할까요?
    * 우선 로(ρ) 햇은 중간층의 유닛의 평균 활성도의 추정치를 나타내고 로(ρ)는 그 목표치가 되는 파라미터이다.
    * 원래의 오차함수 E(w)에 최소 규제화 항을 추가한 E'(W)를 최소화하면 중간층의 각 유닛 평균 활성도가 작은 ρ에 가깝게 되고, 입력의 재현오차 E(w)가 작아지도록 w가 정해진다.
    * 베타(β)는 이 두가지 목표의 밸런스를 결정하는 파라미터다.
* Q. 그렇다면 희소규제화 항에서의 **베타(β)** 에 따라 어떤 특징이 나타날까요?
    * MNIST를 예로 들면 β가 0, 즉 희소 규제화를 하지 않은 경우에 학습된 특징은 어수선한 패턴을 보입니다.
    * 이에 반해 β = 3.0 정도의 강한 규제화를 한다면 각각의 숫자가 그대로 특징으로 선택되고 맙니다.
    * 즉, 학습 시의 희소 규제화는 오토인코더의 중간층의 각 유닛을 '분담'하여 표현하는 양상을 제어하는 역할을 한다고 할 수 있습니다.
    * 희소 규제화를 하지 않을 때는 중간층의 유닛은 각각 제멋대로 입력을 표현하려고 하지만, 알맞은 정도로 희소 규제화가 행해지면, 입력이 갖는 구조를 효율적으로 표현할 수 있도록
    중간층의 유닛이 협력하여 각각의 입력을 표현하게 됩니다. 또, 희소 규제화가 너무 강하면 중간층 유닛이 집합으로서가 아닌, 되도록 단독으로 각각의 입력을 표현하려고 하는 경향이 있습니다.
* Q. **희소규제화**와 **가중치 감쇠**의 차이점은?
    * 가중치 감쇠의 규제화 항은 가중치 파라미터 그 자체에 대한 함수이므로 가중치의 업데이트 식만 수정하면 된다.
    * 하지만, 희소 규제화 항의 경우는 **해당 층 유닛의 활성도에 대한 제약**이기 때문에, 단지 그 충뿐 아니라 해당 층 아래에 있는 모든 층의 파라미터에 의해 정해지게 된다.
    * 따라서 일반적으로는 델타의 역전파 자체를 수정해야 한다는 차이점이 있다.
* Q. 배치 최적화와 미니배치에서 평균 활성도를 어떻게 구할까요?
    * 배치 최적화에서는 모든 샘플의 feed-forward 계산을 한 번 해서 각 유닛의 출력을 구한 뒤 활성도의 평균을 계산해야 합니다.
    * 하지만 미니배치를 사용하여 학습 중이라면, 평균 활성도만을 계산하기 위해 모든 샘플을 계산하는 것이 비효율적이므로 미니배치 내의 모든 샘플에 대해서만 평균 활성도를 구하는 것을 반복합니다.
* Q. **데이터의 백색화(whitneing)** 가 무엇입니까?
    * Trainning 데이터의 불필요한 경향은 학습을 방해하는 원인이 되는데, 학습 전 이를 처리하는 것이 **백색화(whitening)** 라고 합니다. (정규화와 비슷하지만 수준이 더 높음)
    * 백색화는 오토인코더가 좋은 자질을 학습할 수 있을지를 크게 좌우할 수 있습니다.
    * Q-1. 그럼 백색화를 왜 할까요?
        * 훈련 샘플에서 성분 간의 상관성을 제거하려고 백색화를 합니다.
    * Q-2. 정규화랑은 무슨 차이일까요?
        * 정규화는 단위 처리였던 데에 비해, 백색화는 성분 간의 관계를 수정하는 처리입니다.
* Q. **PCA 백색화**와 **ZCA 백색화**는 무엇인가요?
    * 공분산행렬의 고유벡터를 이용하는 것이 샘플 집합에 대한 주성분 분석(PCA)과 일맥상통한다는 점에서 P를 PCA 백색화라고 해요.
    * (P^T)P= Φ를 만족하는 P를 대칭행렬(P=P^T)로 제한하는 방법이 ZCA 백색화라 합니다.
        * 그리고 어떤 백색화를 사용하는 경우에도 데이터에 따라 특정 성분의 분산이 매우 작거나 극단적으로 0이 되는 경우가 있어서 작은 값(ε)을 사용합니다.
        * 조금 더 설명하자면, ZCA 백색화는 각 열벡터(이미지 필터)가 하나하나 다른 픽셀을 샘플로 하고, 그 **픽셀과 주위 픽셀과의 차이를 강조하는 효과(온센터 : on-center)** 를 갖고 있습니다.
        * 데이터 정규화만 했을 때보다 추가적으로 ZCA 백색화를 거치면 훨씬 섬세한 패턴이 학습됩니다.
    * Q-1. PCA 백색화와 ZCA 백색화의 차이점은?
        * ZCA 백색화를 거친 샘플은 직류성분이 제거되어 이미지의 모서리가 강조되어 있는데 반해,
        PCA 백색화를 거친 샘플은 고주파성분이 이미지 전체로 강조되어 원래 이미지의 공간구조가 거의 남지 않은 것처럼 보여져요.
* Q. 한번 더 물을게요. DNN에서 **사전훈련(pre-training)** 을 왜 하죠?
    * 여러 층으로 구성된 feed-forward 신경망은 기울기 소실(gradient vanishing) 문제로 인해 일반적으로는 학습이 잘 되지 않기 때문에 여러 방법 중 하나로 사전훈련을 합니다.
    * Q-1. **사전훈련(pre-training)** 의 기본적인 아이디어는 뭐죠?
        * feed-forward 신경망의 지도 학습법에서는 일반적으로 학습을 시작할 때의 초기 가중치를 랜덤값으로 초기화하는데, 이 초기값을 좀 더 좋은 값으로 정하는 것을 목표로 해요.
* 다양한 사전훈련(pre-training) 중에서 가장 기본적인 **오토인코더**를 사용한 방법의 **절차**를 설명해주세요.
    #### ◎ pre-training 과정
    * 1. 다층 신경망을 한 층씩 여러 개의 단층 신경망으로 분할합니다.
    * 2. 분할한 단층 신경망을 입력층에서 가까운 순서대로 오토인코더와 같은 방법으로 비지도 학습을 수행하여 각 층의 파라미터를 결정합니다.
        * 구체적으로는, 훈련 데이터 {Xn}을 학습시켜 신경망의 가중치 W^(2)와 바이어스 b^(2)를 결정합니다.
    * 3. 학습한 파라미터를 이 단층 신경망에 설정한 상태에서 훈련 데이터 {Xn}을 입력하여 그 출력층의 표현 {Zn^(2)}를 구합니다.
    * 4. 그 인접층을 포함하는 단층 신경망을 마찬가지로 오토인코더를 만들고, 이번에는 {Zn^(2)}를 훈련 샘플로하여 비지도 학습을 수행해 W^(3)와 b^(3)를 학습합니다.
    * 5. 학습한 파라미터를 이 단층 신경망에 설정한 후, {Zn^(2)}를 입력하여 그 출력층의 표현 {Zn^(3)}을 구합니다.
    * 6. 이 과정을 층수만큼 상위층으로 올라가는 순서대로 반복한다. [이 과정을 통해 각 층의 가중치와 바이어스를 얻을 수 있음]
    * => 이러한 오토인코더를 층층이 쌓은 것을 **적층 자기부호화기(stacked autoencoder)** 라고 부릅니다.

    #### ◎ pre-training 이후 지도 학습
    * 사전훈련을 통해 얻은 파라미터를 원래의 신경망 초기값으로 설정한 뒤, 출력층을 포함하는 새로운 한 개 이상의 층을 신경망의 최상위에 추가한다.
    * 이후, 추가된 층의 가중치는 랜덤하게 초기화한 후에 당초의 목표였던 **지도 학습**을 수행한다.(ex 분류문제라면 출력층의 활성화 함수는 소프트맥스)
    * => 이렇게 사전훈련으로 얻은 파라미터를 초기값으로 사용한다면, 신경망의 가중치를 랜덤값으로 초기화했을 때보다 **기울기 소실 문제**가 발생할 가능성이 훨씬 작아지고 학습이 잘 진행된다.
* 사전훈련이 잘 기능하는 이유는 무엇이라고 생각하시나요?
    * x의 벡터공간 내의 **데이터 {Xn}의 분포**를 오토인코더의 **비지도 학습**으로 잘 포착했기 때문이라고 추측해 볼 수 있습니다
* 그외에 **심층 오토인코더(deep autoencoder** 가 있는데 이게 뭐죠?
    * 단층 feed-forward 신경망이 아니라 더 많은 층으로 구성된 오토인코더를 뜻합니다.
* **디노이징 오토인코더(denoising autoencoder)** 란 무엇인가요?
    * 디노이징 오토인코더는 오토인코더를 확장한 것으로, 학습 시에 확률적인 요소를 도입하여 결과적으로는 RBM에 맞먹는 성능을 가지게 됩니다.
    * 또한, 신경망의 구조 자체는 보통 오토인코더와 완전히 같지만, 학습 방법이 다음과 같이 일부 다릅니다.
        * 첫째, 훈련 샘플 x를 확률적으로(랜덤하게) 변동시켜 x'를 얻습니다.
            * ex) 가우스 분포를 따르는 랜덤 노이즈를 더하여 x' = x + δx 로 만듦
        * 둘째, 이렇게 만든 x'를 오토인코더에 입력하고, 부호화(인코더) 및 복호화(디코더)를 거쳐 얻은 출력이 노이즈를 더하기 전 원래 샘플 x에 가까워지도록 학습을 진행합니다.
        * => **보통의 오토인코더에서는 출력이 입력 자체에 가깝게 되도록 학습하기 때문에 x'가 입력이라면 목표 출력도 항상 x'이어야 합니다.(이 부분이 일반 오토인코더와의 차이점)**
            * **그 이외에는 동일** : 출력층의 활성화 함수가 항등사상 -> 제곱오차를 오차함수로 사용, 시그모이드 함수 -> 교차엔트로피를 오차함수로 사용.
            * 이 오차가 작아지도록 신경망의 가중치와 바이어스를 확률적 경사 하강법으로 구합니다.
            * 이때 항상 xn을 조금 변동시킨 x'n을 제시하면서 원래의 xn이 출력되도록 신경망을 훈련시킵니다.
            * 훈련 후에는 입력을 재현할 수 있는 것뿐만 아니라 **노이즈를 제거하는 능력**을 기대할 수 있으며, 이것이 이 이름의 유래가 되었습니다.

</br>

## *※ STEP 6 : 합성곱 신경망(Convolutional Neural Network)*
### 합성곱 신경망의 개요
* 합성곱 신경망은 **합성곱층**과 **풀링층**이라는 특별한 두 종류의 층을 포함하는 feed-forward 신경망으로, 주로 이미지 인식에서 사용됩니다.
* 공통점 : 이전 feed-forward 신경망과 마찬가지로, 역전파법과 확률적 경사 하강법을 사용하여 최적화를 수행합니다.
* 차이점 : 합성곱 신경망의 특징은 **국소 감수 영역(local receptive field)** 및 **가중치 공유**라 불리는 특별한 **층간 결합**을 갖는 것입니다.
    * 이전 feed-forward 신경망은 인접층의 유닛이 모두 서로 연결된(전결합, fully-connected) 것이었다.
    * 이에 비해 CNN은 인접한 층과 층 사이에 특정한 유닛만이 결합을 갖는 특별한 층을 갖게 되고 이러한 층에서 합성곱과 풀링이라는 이미지 처리의 기본적 연산을 하게 됩니다.

* Q. CNN의 전형적인 구조는 어떻게 되죠?
    * Input - **[ 합성곱층(n개) - 풀링 ] 1쌍** - LCN(국소 콘트라스트 정규화) - **[ 합성곱층(n개) - 풀링 ] 1쌍** - 전결합층(fully-connected layer) - 전결합층 - softmax - Output 입니다.
        * 합성곱층과 풀링층이 나오는 형태로 쌍을 이루며 여러 번 반복됩니다. 그리고 국소 콘트라스트 정규화(LCN)층을 배치하는 경우도 있습니다.
        * 합성곱층과 풀링층이 반복되는 구조 뒤에는 인접한 층 사이의 모든 유닛이 결합한 층이 배치되는데 이를 **전결합층(fully-connected layer)** 라고 하며, 일반 feed-forward 신경망의 각 층과 같습니다.
        * 전결합층은 일반적으로 여러 층을 연결하여 배치하고 마지막 출력층은 일반 feed-forward 신경망과 같습니다. [ 분류 목적이면 마지막 출력층 softmax ]
* Q. **합성곱 계산**은 어떻게 하죠?
    * 이미지의 합성곱이란, 이미지와 필터 사이에 정의되는 연산입니다.
    * 이미지의 픽셀값과 필터(작은 크기의 이미지)의 픽셀값을 곱한 것이 이미지의 합성곱입니다.
    * 즉, 이미지에 필터를 겹쳤을 때, 이미지와 필터가 겹쳐지는 픽셀끼리 곱을 구한 다음 필터 전체에서 그 값을 합하는 연산을 뜻합니다.
    * **참고** : 결과 크기 = 입력크기 - 필터크기 + 1
* Q. **합성곱**은 어떠한 작용을 하나요?
    * 이미지의 합성곱은 필터의 명암 패턴과 유사한 명암 패턴이 입력된 이미지에서 어디에 있는지를 검출하는 작용을 합니다.
    * 간단히 말하면, 합성곱 연산은 이미지 처리에서 말하는 필터연산에 해당합니다. 
* Q. **제로 패딩(zero-padding)**은 뭔가요?
    * 제로 패딩을 설명하려면 패딩을 먼저 설명할게요.
    * 보통 합성곱한 결과 이미지가 입력 이미지와 크기가 같으면 편리할 때가 많아요.
    * 입력 이미지의 바깥쪽에 폭만큼 테두리를 둘러 크기를 늘려서 출력 이미지의 크기가 원래 입력 이미지와 같은 크기가 되도록 만드는 것입니다.
    * 그 중에서도 제로 패딩은 이 '테두리' 부분의 픽셀값을 0으로 설정하는 방법입니다.
    * 제로 패딩 처리를 하면 출력 이미지의 주변부가 어둡게 될 수 있어요.
* Q. **스트라이드(stride)**란 무엇인가요?
    * 필터를 적용하는 위치의 간격을 스트라이드(stride)라고 합니다. 
    * 스트라이드를 키우면 출력 크기는 작아지지만, 너무 큰 값을 지정하는 것은 이미지의 특징을 놓칠 가능성이 있기 때문에 주의해야 합니다.
* Q. **합성곱층의 특징**이 뭐죠?
    * 국소적인 결합을 가지는 것과 가중치를 공유하는 것입니다.
        * 국소적 결합 : 합성곱 연산의 국소성을 반영하여 출력층의 유닛 하나(채널 m의 한 개 픽셀)는 입력층의 유닛하고만 결합한다는 뜻입니다.
        * 가중치 공유 : 결합의 가중치는 출력층의 같은 채널에 속하는 모든 유닛에서 같다는 뜻입니다.  
* Q. **풀링층** 이란 무엇인가요?
    * 풀링층은 보통 합성곱층의 바로 뒤에 배치됩니다.
    * 합성곱층에서 추출한 feature의 위치 감도를 약간 저하시켜 대상이 되는 feature값의 이미지 내에서의 위치가 조금씩 변화하는 경우에도 풀링층의 출력이 변화하지 않도록 해주는 역할을 합니다.

* Q. **최대 풀링(max pooling)** 과 **평균 풀링(average pooling)** 을 설명해주세요.
    * 적당히 패딩을 적용해 입력 이미지의 가장자리를 포함하고 모든 점을 중심으로 하는 P_ij를 만들 수 있고,
     이 P_ij 내의 픽셀에 대해서 채널 k마다 독립적으로 H^2개 있는 픽셀값을 사용하여 하나의 픽셀값 u_ijk를 구할 수 있습니다.
    * 그 중에서 **최대 풀링**은 H^2개의 픽셀값 중 최대값을 고르는 방법, **평균 풀링**은 픽셀값의 평균을 계산하여 픽셀값으로 사용하는 방법을 의미합니다.
* Q. 풀링층의 특징을 설명해주세요.
    * 합성곱층과 마찬가지로 풀링층에도 2 이상의 스트라이드를 설정할 수 있으며 통상 이렇게 설정합니다.
    * 또한 합성곱층과 마찬가지로, 2층 신경망으로도 구성할 수 있고 층간의 결합이 국소적으로 제한되도록 구성된다.
        * **BUT** 이 경우 결합의 가중치는 합성곱층의 필터처럼 고정값으로 되어 있어 조절할 수 없다.
        * 따라서 풀링층에는 학습에 따라 변화할 수 있는 파라미터가 존재하지 않는다. 역전파법을 수행할 때 풀링층에는 델타에 대한 역전파 계산만을 수행한다.
* Q. 정규화층에 사용되는 방법과 **국소 콘트라스트 정규화(local contrast normalization)** 에 대해서 설명해주세요.
    * 명암을 여러 방법으로 정규화하는 방법 중에 이전에 설명해드렸던 것은, 이미지의 집합(훈련 데이터)에 대한 통계치를 이용하는 방법이 있었는데 정규화나 백색화가 있습니다.
    * CNN에서는 대상 이미지로부터 학습 이미지의 픽셀 단위 평균을 뺀 다음 CNN의 입력으로 사용하는 경우가 있습니다.
    * 이러한 방법 이외에 **국소 콘트라스트 정규화**가 있는데, 이 방법은 이미지 한장 한장에 대해 개별적으로 처리하는 방법입니다.[**국소 영역 내의 모든 채널의 픽셀을 대상으로 평균과 분산을 계산**]
        * CNN뿐만 아니라 일반적인 이미지 처리 방법 중 하나 입니다. 합성곱 연산이나 풀링과 마찬가지로 한 개 층으로 이 처리를 구현할 수 있고 역전파 계산도 가능합니다.
        * 하지만 풀링층과 마찬가지로 이층의 가중치는 고정값이어서 학습이 가능한 파라미터가 없습니다.
        * 국소 콘트라스트 정규화에는 **감산 정규화(subtractive normalization)** 와 **제산 정규화(divisive normalization)** 두 가지가 있습니다.
* Q. 그러면 감산 정규화는 무엇인가요?
    * 감산 정규화는 입력 이미지의 각 픽셀 명암에서 P_ij(H x H 크기의 정사각형 모양의 영역)에 포함되는 **픽셀의 명암값의 평균**을 빼는 것을 의미합니다.
    * 이는 영역의 중앙부를 중요시하고 주변부의 영향력을 상대적으로 줄이기 위한 방법입니다. [영역의 중앙부에서 값이 최대가 되고 주변으로 갈수록 작아짐]
* Q. 제산 정규화는요?
    * 제산 정규화도 마찬가지로 국소 영역 내에서 이루어지지만 픽셀값의 분산을 추가적으로 필요로 합니다.
    * 감산 정규화를 거친 입력 이미지의 픽셀값을 다시 **분산의 제곱근**인 **표준편차**로 나누어줍니다.
    * 감산 정규화를 그대로 하게 되면 명암의 차이가 적은(대비가 적은) 국소 영역일수록 작은 명암의 차이가 증폭되어서 이미지의 노이즈가 강조되는 결과가 일어납니다.
        * 따라서 입력 이미지 내에서 대비가 큰 부분에만 적용되도록 처리를 해줍니다.
    

