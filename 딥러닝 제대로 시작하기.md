BOOK : 딥러닝 제대로 시작하기
===========

## *※ STEP 1*

## *※ STEP 2*

* Q. 기본적인 신경망 계산 과정을 간단하게 설명해주세요
    * 훈련 샘플을 입력하는 feed-forward(앞먹임) 계산을 하고 오차를 구한 뒤, 이어 역전파 계산을 하고 오차의 기울기를 구하여 가중치를 업데이트합니다.
## *※ STEP 3*
* Q. **과적합**이란 무엇인가?
    * A. 과적합이란, 학습 시에 오차함수의 값이 작은 극소점에 갇힌 상황이다.
* Q. 과적합을 완화시키는 방법에는 무엇이 있죠?
    * A. 규제화(regularization)의 방법에는 가중치 감쇠(weight decay), 가중치 상한,  드롭아웃(drop-out) 등이 있어요.
    * Q-1. 왜 위와 같은 방법들이 제안되었어요?
        * 신경망의 자유도를 낮추는 것은, 과다한 자유도를 가질 경우를 제외하고는 그다지 바람직하지 않아요. 그래서 학습 시에 가중치의 자유도를 제약하는 규제화에 의해 과적합 문제를 완화하도록 몇 가지 방법이 제안되었어요.
    * Q-2. **가중치 감쇠(weight decay)**는 무엇이죠?
        * 가장 간단한 규제화 방법은 가중치에 어떤 제약을 가하는 거에요. 그중에서도 오차함수에 가중치의 제곱합(norm의 제곱)을 더한 뒤, 이를 최소화하는 방법이 가중치 감쇠에요.
        * 추가로, 람다는 이 규제화의 강도를 제어하는 파라미터인데요, 이 람다를 추가하여 가중치는 자신의 크기에 비례하는 속도로 항상 감쇠하도록 업데이트돼요.
    * Q-3. **가중치 상한**은요?
        * 각 유닛의 입력 측 결합의 가중치에 대해서 그 제곱합의 최댓값을 제약하는 방법이에요.
        * 만약에 가중치 상한 부등식을 만족하지 않는다면, 가중치에 미리 정한 (1보다 작은) 상수를 곱하여 부등식을 만족하도록 만들어요.
    * Q-4. 마지막으로 **드롭아웃(drop-out)**이 무엇이죠?
        * 다층(multi-layer) 신경망의 유닛 중 일부를 확률적으로 선택하여 학습하는 방법이에요.
        * 자세히 말하면, 중간층과 입력층 각 층의 유닛 중 미리 정해 둔 비율 p만큼을 선택하고 선택되지 않은 유닛을 무효화 취급합니다.
        * 즉, 가중치를 업데이트할 때마다 다시 무작위로 선택되며 학습이 끝나고 추론 시에는 모든 유닛을 사용하여 feed-forward(앞먹임) 계산을 합니다.
        * 드롭아웃의 대상이 되었던 층의 유닛은 모든 출력을 p배로 합니다.(또는 출력의 가중치를 p배로 해요.)
            * 그 이유는 추론시의 유닛 수가 학습 시에 비해 1/p배 된 것과 같기 때문에 이를 보상하기 위함이에요.
        * 특히, 클래스 분류의 출력층에 사용되는 소프트맥스 층에 대해서는 드롭아웃 방법이 출력의 기하평균을 내는 것과 같음을 알 수 있어요.
    * Q-5. RBM이나 오토인코더의 학습에 드롭아웃을 적용하면 어떤 효과를 얻을 수 있죠?
        * 두 경우 모두 희소적 특징이 자동 학습된다는 결과가 보고되었어요. 즉, 희소 규제화를 거치지 않고도 유사한 특징을 얻을 수 있다는 이야기죠.
    * Q-6. 드롭아웃과 비슷한 방법이 무엇이 있죠?
        * 드롭 커넥트와 확률적 최대 풀링이 있어요.
* Q. **데이터 정규화(normalization of data)**가 뭐에요?
    * 각 샘플 Xn을 선형 변환하는건데 (Xn-X평균)/표준편차로 구할 수 있어요.
    * 데이터 정규화를 거친 샘플은 각 성분의 평균이 0, 분산이 1이 돼요.
    * 특정한 성분의 분산이 0이거나 매우 작은 경우가 있으면 max(표준편차,충분히 작은값)으로 나누어줘요.
* Q. **데이터의 확장(data augmentation)**이란 무엇이죠?
    * 데이터 확장은요 샘플 데이터를 일정하게 가공해서 양적으로 '물타기'하는 작업이라고 해요.
        * 확장을 하는 이유는 트레이닝 데이터의 부족이 과적합을 일으키는 가장 큰 원인이기 때문이에요.
    * 이미지 데이터같이 샘플의 분포 양상을 예상할 수 있는 경우에 특히 유효해요. 예를 들어서 같은 카테고리의 물체 이미지이기만 하면 어느 정도 변화를 가해도 타당한 훈련 샘플이라고 보기 때문이에요.
    * 가우스 분포를 따르는 **랜덤 노이즈**를 일괄적으로 적용하는 방법도 유효한 방법이에요. 
 * Q. **학습률(Learning Rate)**은 어떻게 결정하고 그것이 왜 중요한가요?
    * 경사 하강법에서는 파라미터의 업데이트 정도를 학습률을 통해 조절해요. 학습률 설정은 학습의 성패를 좌우할 정도로 중요해요.
    * 학습률을 결정하는데에 정석과 같은 두 가지 방법이 있어요.
        * 첫번째는, 학습 초기에 값을 크게 설정했다가 학습을 하면서 학습률을 점점 줄여가는 방법이에요.
        * 두번째는, 신경망의 모든 층에서 같은 학습률을 사용하는 것이 아니라 층마다 서로 다른 값을 사용하는 방법이에요.
            * 예를 들어, 출력 방향에 가까운 얕은 층에서는 학습률을 작게 잡고, 입력에 가까운 깊은 층에는 크게 잡는 경우가 있어요.
            * 또한, CNN처럼 가중치 공유를 할 때 학습률을 가중치가 공유되는 수의 제곱근에 비례하도록 설정하면, 가중치의 업데이트 속도를 대체로 비슷하게 맞출 수 있어요.
* Q. 위의 질문과는 달리 학습률을 자동으로 결정하는 방법에는 뭐가 있죠?
    * AdaGrad가 있어요. 이거는 직관적으로, 자주 나타나는 기울기의 성분보다 드물게 나타나는 기울기 성분을 더 중시해서 파라미터를 업데이트하는 방법이에요.

## *※ STEP 4*
* Q. 역전파법(backpropagation)이 뭔가요?
    * 역전파란 앞먹임 신경망의 학습에서는 가중치와 바이어스에 대한 오차함수의 미분을 계산해야하는데 이러한 미분을 효율적으로 계산하는 방법이에요.
* Q. 기울기 벡터의 각 성분은 뭐죠?
    * 각 층의 결합 가중치(w)와 각 유닛의 바이어스(b)에 대한 오차함수의 편미분이에요.
    * 