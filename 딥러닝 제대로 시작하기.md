BOOK : 딥러닝 제대로 시작하기
===========

## *※ STEP 1*

</br >

## *※ STEP 2*

* Q. 기본적인 신경망 계산 과정을 간단하게 설명해주세요
    * 훈련 샘플을 입력하는 feed-forward(앞먹임) 계산을 하고 오차를 구한 뒤, 이어 역전파 계산을 하고 오차의 기울기를 구하여 가중치를 업데이트합니다.
</br >

## *※ STEP 3*
* Q. **과적합**이란 무엇인가?
    * A. 과적합이란, 학습 시에 오차함수의 값이 작은 극소점에 갇힌 상황이다.
* Q. 과적합을 완화시키는 방법에는 무엇이 있죠?
    * A. 규제화(regularization)의 방법에는 가중치 감쇠(weight decay), 가중치 상한,  드롭아웃(drop-out) 등이 있어요.
    * Q-1. 왜 위와 같은 방법들이 제안되었어요?
        * 신경망의 자유도를 낮추는 것은, 과다한 자유도를 가질 경우를 제외하고는 그다지 바람직하지 않아요. 그래서 학습 시에 가중치의 자유도를 제약하는 규제화에 의해 과적합 문제를 완화하도록 몇 가지 방법이 제안되었어요.
    * Q-2. **가중치 감쇠(weight decay)** 는 무엇이죠?
        * 가장 간단한 규제화 방법은 가중치에 어떤 제약을 가하는 거에요. 그중에서도 오차함수에 가중치의 제곱합(norm의 제곱)을 더한 뒤, 이를 최소화하는 방법이 가중치 감쇠에요.
        * 추가로, 람다는 이 규제화의 강도를 제어하는 파라미터인데요, 이 람다를 추가하여 가중치는 자신의 크기에 비례하는 속도로 항상 감쇠하도록 업데이트돼요.
    * Q-3. **가중치 상한**은요?
        * 각 유닛의 입력 측 결합의 가중치에 대해서 그 제곱합의 최댓값을 제약하는 방법이에요.
        * 만약에 가중치 상한 부등식을 만족하지 않는다면, 가중치에 미리 정한 (1보다 작은) 상수를 곱하여 부등식을 만족하도록 만들어요.
    * Q-4. 마지막으로 **드롭아웃(drop-out)** 이 무엇이죠?
        * 다층(multi-layer) 신경망의 유닛 중 일부를 확률적으로 선택하여 학습하는 방법이에요.
        * 자세히 말하면, 중간층과 입력층 각 층의 유닛 중 미리 정해 둔 비율 p만큼을 선택하고 선택되지 않은 유닛을 무효화 취급합니다.
        * 즉, 가중치를 업데이트할 때마다 다시 무작위로 선택되며 학습이 끝나고 추론 시에는 모든 유닛을 사용하여 feed-forward(앞먹임) 계산을 합니다.
        * 드롭아웃의 대상이 되었던 층의 유닛은 모든 출력을 p배로 합니다.(또는 출력의 가중치를 p배로 해요.)
            * 그 이유는 추론시의 유닛 수가 학습 시에 비해 1/p배 된 것과 같기 때문에 이를 보상하기 위함이에요.
        * 특히, 클래스 분류의 출력층에 사용되는 소프트맥스 층에 대해서는 드롭아웃 방법이 출력의 기하평균을 내는 것과 같음을 알 수 있어요.
    * Q-5. RBM이나 오토인코더의 학습에 드롭아웃을 적용하면 어떤 효과를 얻을 수 있죠?
        * 두 경우 모두 희소적 특징이 자동 학습된다는 결과가 보고되었어요. 즉, 희소 규제화를 거치지 않고도 유사한 특징을 얻을 수 있다는 이야기죠.
    * Q-6. 드롭아웃과 비슷한 방법이 무엇이 있죠?
        * 드롭 커넥트와 확률적 최대 풀링이 있어요.
* Q. **데이터 정규화(normalization of data)** 가 뭐에요?
    * 각 샘플 Xn을 선형 변환하는건데 (Xn-X평균)/표준편차로 구할 수 있어요.
    * 데이터 정규화를 거친 샘플은 각 성분의 평균이 0, 분산이 1이 돼요.
    * 특정한 성분의 분산이 0이거나 매우 작은 경우가 있으면 max(표준편차,충분히 작은값)으로 나누어줘요.
* Q. **데이터의 확장(data augmentation)** 이란 무엇이죠?
    * 데이터 확장은요 샘플 데이터를 일정하게 가공해서 양적으로 '물타기'하는 작업이라고 해요.
        * 확장을 하는 이유는 트레이닝 데이터의 부족이 과적합을 일으키는 가장 큰 원인이기 때문이에요.
    * 이미지 데이터같이 샘플의 분포 양상을 예상할 수 있는 경우에 특히 유효해요. 예를 들어서 같은 카테고리의 물체 이미지이기만 하면 어느 정도 변화를 가해도 타당한 훈련 샘플이라고 보기 때문이에요.
    * 가우스 분포를 따르는 **랜덤 노이즈**를 일괄적으로 적용하는 방법도 유효한 방법이에요. 
 * Q. **학습률(Learning Rate)** 은 어떻게 결정하고 그것이 왜 중요한가요?
    * 경사 하강법에서는 파라미터의 업데이트 정도를 학습률을 통해 조절해요. 학습률 설정은 학습의 성패를 좌우할 정도로 중요해요.
    * 학습률을 결정하는데에 정석과 같은 두 가지 방법이 있어요.
        * 첫번째는, 학습 초기에 값을 크게 설정했다가 학습을 하면서 학습률을 점점 줄여가는 방법이에요.
        * 두번째는, 신경망의 모든 층에서 같은 학습률을 사용하는 것이 아니라 층마다 서로 다른 값을 사용하는 방법이에요.
            * 예를 들어, 출력 방향에 가까운 얕은 층에서는 학습률을 작게 잡고, 입력에 가까운 깊은 층에는 크게 잡는 경우가 있어요.
            * 또한, CNN처럼 가중치 공유를 할 때 학습률을 가중치가 공유되는 수의 제곱근에 비례하도록 설정하면, 가중치의 업데이트 속도를 대체로 비슷하게 맞출 수 있어요.
* Q. 위의 질문과는 달리 학습률을 자동으로 결정하는 방법에는 뭐가 있죠?
    * AdaGrad가 있어요. 이거는 직관적으로, 자주 나타나는 기울기의 성분보다 드물게 나타나는 기울기 성분을 더 중시해서 파라미터를 업데이트하는 방법이에요.
* Q. **모멘텀(momentum)** 을 설명해주세요.
    * 모멘텀은 경사 하강법의 수렴 성능을 향상시키기 위한 방법 중 하나인데요, 가중치의 업데이트 값에 이전 업데이트 값의 일정 비율을 더하는 방법이에요.
    * Q-1. 모멘텀의 효과는 무엇이죠?
        * 오차함수가 깊은 골짜기 같은 형상을 가지는데, 골짜기 바닥이 평평할 때 약간만 빠져나와도 골짜기와 직교하는 방향으로 큰 기울기가 생기면서 바닥을 정상적으로 탐색하지 못하는 문제가 있어요
        * 이러한 문제를 해결하고 골짜기 방향을 따라 골짜기 바닥을 효율적으로 탐색할 수 있게 만들어주죠.
* Q. **가중치의 초기화** 하는 방법을 말해주세요.
    * 가장 일반적인것은 가우스 분포로부터 랜덤값을 생성하여 초기값으로 삼는 방법이에요. 그리고 가우스 분포의 표준편차 선택도 중요한데,
     보통 표준편차를 크게 잡으면 초기 학습은 빠르게 진행되지만, 오차함수의 감소가 일찌감치 멈춰버리는 경향이 있을 수 있어요.
    * 

</br >

## *※ STEP 4* [책보기]
* Q. **역전파법(backpropagation)** 이 뭔가요?
    * 역전파란 앞먹임 신경망 학습에서는 가중치와 바이어스에 대한 오차함수의 미분을 계산해야하는데 이러한 미분을 효율적으로 계산하는 방법이에요.
    * Q-1. 역전파법을 왜 사용하죠?
        * **경사 하강법**을 실행하기 위해서는 오차함수 E(w)의 기울기를 계산해야 하는데, 이 미분의 계산이 매우 까다롭기 때문에 역전파법을 사용하는거죠.
        * 각 층의 결합 가중치(w)와 각 유닛의 바이어스(b)에 대한 오차함수의 편미분이 기울기 벡터의 각 성분이고,
        자세히는 중간층, 특히 입력이 가까운 깊은 층의 파라미터일수록 미분을 계산하기 까다로워요.
* Q. 오차 역전파를 통해 오차 기울기(가중치에 대한 오차의 미분)를 계산하는 절차를 말해줘요.
    * 1. 각각의 층의 유닛 입력 u과 출력 z을 순서대로 계산한다.
    * 2. 출력층 델타(δ)를 구한다. (통상적으로 δ = z - d : 출력층 L의 유닛 j의 델타 δ는 신경망의 출력(z)과 목표 출력(d)의 차가 된다.)
    * 3. **역전파** : 각 중간층 l( = L-1, L-2, .., 2)에서의 델타 δ를 출력층부터 가까운 순서대로 계산한다.
    * 4. 각 층 l(= 2, ..., L))의 파라미터 w에 대해 미분을 계산한다.
    * **참조** : l-1번째 층의 유닛 i와 l번째 층의 유닛 j를 잇는 결합의 가중치 w_ji에 대한 미분은,
    유닛 j에 대한 델타(δ_j)(L)와 유닛 i의 출력 z_i(L-1)의 곱에 지나지 않는다.
* Q. 순전파와 역전파 계산의 **공통점과 차이점**은?
    * 공통점 : 순전파와 역전파 계산은 모두 층 단위의 행렬 계산으로 나타낼 수 있으며 식의 형태가 닮았다는 공통점이 있다.
    * 차이점 : 순전파는 비선형 계산인데 비해, 역전파는 선형 계산이라는 차이점이 있다.
        * 순전파 계산에서는 각 층에 대한 입력은 유닛이 갖는 활성화 함수를 경유하기 때문에, 활성화 함수가 비선형이라면 이 층의 입출력의 관계도 비선혀성을 갖는다.
            * ex) 로지스틱 함수를 예로 들면 각 층의 출력은 항상 [0, 1]의 범위로 제약되며, 값이 지나치게 커져서 발산해 버리는 일은 일어나지 않는다.
        * 한편, 역전파 계산은 선형 계산이다. 그 결과, 각 층의 가중치의 값이 크면 델타가 각 층을 거쳐 전달되는 도중에 **급속하게 커지거나(발산)**, 혹은 반대로 기울기가 작으면 **급속하게 작아져 0(소실)** 이 되어 버린다. 어떤 경우든 가중치의 업데잍트가 잘안되며 학습 자체가 어려워진다.
* 