BOOK : 딥러닝 제대로 시작하기
===========

## *※ STEP 1*

</br >

## *※ STEP 2*

* Q. 기본적인 신경망 계산 과정을 간단하게 설명해주세요
    * 훈련 샘플을 입력하는 feed-forward(앞먹임) 계산을 하고 오차를 구한 뒤, 이어 역전파 계산을 하고 오차의 기울기를 구하여 가중치를 업데이트합니다.
</br >

## *※ STEP 3*
* Q. **과적합**이란 무엇인가?
    * A. 과적합이란, 학습 시에 오차함수의 값이 작은 극소점(local minimum)에 갇힌 상황이다.
* Q. 과적합을 **완화**시키는 방법에는 무엇이 있죠?
    * A. 과적합을 완화시키려면 규제화(regularization)를 해야하는데 방법으론 가중치 감쇠(weight decay), 가중치 상한,  드롭아웃(drop-out) 등이 있어요.
    * Q-1. 왜 위와 같은 방법들이 제안되었어요?
        * 신경망의 자유도를 낮추는 것은, 과다한 자유도를 가질 경우를 제외하고는 그다지 바람직하지 않아요. 그래서 학습 시에 가중치의 자유도를 제약하는 규제화에 의해 과적합 문제를 완화하도록 몇 가지 방법이 제안됐어요.
    * Q-2. **가중치 감쇠(weight decay)** 는 무엇이죠?
        * 가장 간단한 규제화 방법은 가중치에 어떤 제약을 가하는 거에요. 그중에서도 오차함수에 **가중치의 제곱합(norm의 제곱)** 을 더한 뒤, 이를 최소화하는 방법이 가중치 감쇠에요.
        * 추가로, **람다(λ)** 는 이 규제화의 강도를 제어하는 파라미터인데요, 이 람다를 추가하여 가중치는 자신의 크기에 비례하는 속도로 항상 감쇠하도록 업데이트돼요.
    * Q-3. **가중치 상한**은요?
        * 각 유닛의 입력 측 가중치에 대해서 그 제곱합의 최댓값을 제약하는 방법이에요.
        * 만약에 가중치 상한 부등식을 만족하지 않는다면, 가중치에 미리 정한 (1보다 작은) 상수를 곱하여 부등식을 만족하도록 만들어요.
    * Q-4. 마지막으로 **드롭아웃(drop-out)** 이 무엇이죠?
        * 다층(multi-layer) 신경망의 유닛 중 일부를 확률적으로 선택하여 학습하는 방법이에요.
        * 자세히 말하면, 중간층과 입력층 각 층의 유닛 중 미리 정해 둔 비율 p만큼을 선택하고 선택되지 않은 유닛을 무효화 취급합니다.
        * 즉, 가중치를 업데이트할 때마다 다시 무작위로 선택되며 학습이 끝나고 추론 시에는 모든 유닛을 사용하여 feed-forward(앞먹임) 계산을 합니다.
        * 드롭아웃의 대상이 되었던 층의 유닛은 모든 출력을 p배로 합니다.(또는 출력의 가중치를 p배로 해요.)
            * 그 이유는 추론시의 유닛 수가 학습 시에 비해 1/p배 된 것과 같기 때문에 이를 보상하기 위함이에요.
        * 특히, 클래스 분류의 출력층에 사용되는 소프트맥스 층에 대해서는 드롭아웃 방법이 출력의 기하평균을 내는 것과 같음을 알 수 있어요.
    * Q-5. RBM이나 오토인코더의 학습에 드롭아웃을 적용하면 어떤 효과를 얻을 수 있죠?
        * 두 경우 모두 희소적 특징이 자동 학습된다는 결과가 보고되었어요. 즉, 희소 규제화를 거치지 않고도 유사한 특징을 얻을 수 있다는 이야기죠.
    * Q-6. 드롭아웃과 비슷한 방법이 무엇이 있죠?
        * 드롭 커넥트와 확률적 최대 풀링이 있어요.
* Q. **데이터 정규화(normalization of data)** 가 뭐에요?
    * 각 샘플 Xn을 **선형 변환**하는건데 (Xn-X평균)/표준편차로 구할 수 있어요.
    * 데이터 정규화를 거친 샘플은 각 성분의 평균이 0, 분산이 1이 돼요.
    * 특정한 성분의 분산이 0이거나 매우 작은 경우가 있으면 max(표준편차,충분히 작은값)으로 나누어줘요.
* Q. **데이터의 확장(data augmentation)** 이란 무엇이죠?
    * 데이터 확장은요 샘플 데이터를 일정하게 가공해서 양적으로 '물타기'하는 작업이라고 해요.
    * Q-1. 이걸 왜 하는거죠?
        * 확장을 하는 이유는 트레이닝 데이터의 부족이 **과적합**을 일으키는 가장 큰 원인이기 때문이에요.
    * 이미지 데이터같이 샘플의 분포 양상을 예상할 수 있는 경우에 특히 유효해요. 예를 들어서 같은 카테고리의 물체 이미지이기만 하면 어느 정도 변화를 가해도 타당한 훈련 샘플이라고 보기 때문이에요.
    * 가우스 분포를 따르는 **랜덤 노이즈**를 일괄적으로 적용하는 방법도 유효한 방법이에요. 
 * Q. **학습률(Learning Rate)** 이 무엇이며 그것이 왜 중요한가요?
    * 경사 하강법에서는 **파라미터의 업데이트 정도**를 학습률을 통해 조절해요. 학습률 설정은 학습의 성패를 좌우할 정도로 중요해요.
    * Q-1. 학습률은 어떻게 결정하나요?
        * 학습률을 결정하는데에 정석과 같은 두 가지 방법이 있어요.
        * 첫 번째는, 학습 초기에 값을 크게 설정했다가 학습을 하면서 학습률을 점점 줄여가는 방법이에요.
        * 두 번째는, 신경망의 모든 층에서 같은 학습률을 사용하는 것이 아니라 층마다 서로 다른 값을 사용하는 방법이에요.
            * 예를 들어, 출력 방향에 가까운 얕은 층에서는 학습률을 작게 잡고, 입력에 가까운 깊은 층에는 크게 잡는 경우가 있어요.
            * 또한, CNN처럼 **가중치 공유**를 할 때 학습률을 가중치가 공유되는 수의 제곱근에 비례하도록 설정하면, 가중치의 업데이트 속도를 대체로 비슷하게 맞출 수 있어요.
* Q. 위의 질문과는 달리 학습률을 자동으로 결정하는 방법에는 뭐가 있죠?
    * AdaGrad가 있어요. 이거는 직관적으로, **자주 나타나는** 기울기의 성분보다 **드물게 나타나는** 기울기 성분을 더 중시해서 파라미터를 업데이트하는 방법이에요.
* Q. **모멘텀(momentum)** 을 설명해주세요.
    * 모멘텀은 경사 하강법의 **수렴 성능을 향상시키기 위한 방법** 중 하나인데요, 가중치의 업데이트 값에 이전 업데이트 값의 일정 비율을 더하는 방법이에요.
    * Q-1. 모멘텀의 효과는 무엇이죠?
        * 오차함수가 깊은 골짜기 같은 형상을 가지는데, 골짜기 바닥이 평평할 때 약간만 빠져나와도 골짜기와 직교하는 방향으로 큰 기울기가 생기면서 바닥을 정상적으로 탐색하지 못하는 문제가 있어요
        * 이러한 문제를 해결하고 골짜기 방향을 따라 골짜기 바닥을 효율적으로 탐색할 수 있게 만들어주죠.
* Q. **가중치의 초기화** 하는 방법을 말해주세요.
    * 가장 일반적인것은 가우스 분포로부터 랜덤값을 생성하여 초기값으로 삼는 방법이에요. 그리고 가우스 분포의 표준편차 선택도 중요한데, 보통 표준편차를 크게 잡으면 초기 학습은 빠르게 진행되지만, 오차함수의 감소가 일찌감치 멈춰버리는 경향이 있을 수 있어요.

</br >

## *※ STEP 4 : 역전파 * [책보기]
* Q. **역전파법(backpropagation)** 이 뭔가요?
    * 역전파는 앞먹임 신경망 학습에서 가중치와 바이어스에 대한 오차함수의 미분을 계산해야하는데 이러한 미분을 효율적으로 계산하는 방법이에요.
    * Q-1. 역전파법을 왜 사용하죠?
        * **경사 하강법**을 실행하기 위해서는 오차함수 E(w)의 기울기를 계산해야 하는데, 이 미분의 계산이 매우 까다롭기 때문에 역전파법을 사용하는거죠.
        * 각 층의 결합 가중치(w)와 각 유닛의 바이어스(b)에 대한 오차함수의 편미분이 기울기 벡터의 각 성분이고,
        자세히는 중간층, 특히 입력이 가까운 깊은 층의 파라미터일수록 미분을 계산하기 까다로워요.
* Q. 오차 역전파를 통해 **오차 기울기(가중치에 대한 오차의 미분)를 계산하는 절차** 를 말해줘요.
    * 1. 각각의 층의 유닛 입력 u과 출력 z을 순서대로 계산한다.
    * 2. 출력층 델타(δ)를 구한다. (통상적으로 δ = z - d : 출력층 L의 유닛 j의 델타 δ는 신경망의 출력(z)과 목표 출력(d)의 차가 된다.)
    * 3. **역전파** : 각 중간층 l( = L-1, L-2, .., 2)에서의 델타 δ를 출력층부터 가까운 순서대로 계산한다.
    * 4. 각 층 l(= 2, ..., L))의 파라미터 w에 대해 미분을 계산한다.
    * **참조** : l-1번째 층의 유닛 i와 l번째 층의 유닛 j를 잇는 결합의 가중치 w_ji에 대한 미분은,
    유닛 j에 대한 **델타(δ_j)(L)** 와 유닛 i의 **출력 z_i(L-1)** 의 **곱**에 지나지 않는다.
* Q. 순전파와 역전파 계산의 **공통점과 차이점**은?
    * **공통점** : 순전파와 역전파 계산은 모두 층 단위의 행렬 계산으로 나타낼 수 있으며 식의 형태가 닮았다는 공통점이 있다.
    * **차이점** : 순전파는 **비선형 계산**인데 비해, 역전파는 **선형 계산**이라는 차이점이 있다.
        * 순전파 계산에서는 각 층에 대한 입력은 유닛이 갖는 활성화 함수를 경유하기 때문에, 활성화 함수가 비선형이라면 이 층의 입출력의 관계도 비선형성을 갖는다.
            * ex) 로지스틱 함수를 예로 들면 각 층의 출력은 항상 [0, 1]의 범위로 제약되며, 값이 지나치게 커져서 발산해 버리는 일은 일어나지 않는다.
        * 한편, 역전파 계산은 선형 계산이다. 그 결과, 각 층의 가중치의 값이 크면 델타가 각 층을 거쳐 전달되는 도중에 **급속하게 커지거나(발산)**, 혹은 반대로 기울기가 작으면 **급속하게 작아져 0(소실)** 이 되어 버린다. 어떤 경우든 가중치의 업데잍트가 잘안되며 학습 자체가 어려워진다.

</br>

## *※ STEP 5 : 자기부호화기(autoencoder)*
* Q. 오토인코더의 목적이 뭐에요?
    * 오코인코더는 **목표 출력없이 입력만으로** 구성된 트레이닝 데이터로 **비지도 학습**을 수행하여 데이터의 특징을 나타내는, 더 나은 표현을 얻는 것이 목표인 신경망입니다.
    * 또한, 딥 네트워크의 사전훈련(pre-training), 즉 그 **가중치의 좋은 초기값**을 얻는 목적으로 이용됩니다.
    * 자세히 설명하면, feature의 학습을 통해 샘플 x의 또 '다른' 표현인 y를 얻는 것이고, 직관적으로 x를 그대로 쓰는 대신 변환된 y를 사용하는 것입니다.
* Q. 오토인코더의 학습 목표가 뭐죠?
    * 오토인코더의 학습의 목표는 입력을 **부호화(encode)** 한 뒤, 이어 다시 **복호화(decode)** 했을 때 원래의 입력을 되도록 충실히 재현할 수 있는 부호화 방법을 찾는 것이에요.
    * 또한, 오토인코더는 **오차함수를 최소화하는 과정**을 통해 신경망의 가중치와 바이어스를 결정하게 됩니다.
* Q. 오토인코더의 **활성화 함수**와 **오차함수**는 보통 무엇을 사용하죠?
    * 오토인코더의 활성화 함수중에서 중간층의 f는 자유롭게 바꿀수 있으며 통상적으로 **비선형함수**를 사용합니다. 그리고 출력층의 f'은 신경망의 목표 출력이 입력한 x 자신이 될 수 있도록 입력 데이터의 유형에 따라 선택합니다.
    * 1.x의 성분이 실수값을 가질때, 출력층의 f'은 항등사상으로 하는 것이 좋고 오차함수로는 입력값과 출력값에 차에 대한 제곱합을 사용합니다.
    * 2.x의 성분이 이진값을 갖는 경우에는 f'으로 로지스틱 함수를 보통 사용하고 오차함수로는 교차 엔트로피를 사용합니다.
* Q. 오토인코더를 결정하는 요소에는 뭐가 있을까요?
    * 주로 중간층의 유닛 수와 해당층에서 사용되는 활성화 함수가 있죠.
* Q. **과완비(overcomplete)** 란 무엇이죠?
    * 오토인코더는 입력 데이터의 **feature(자질)** 를 학습함으로써, 입력 데이터에서 불필요한 정보를 제거하고 그 본질만을 추출하는 겁니다.
    * 이러한 이유 때문에 입력 데이터의 성분 수 Dx보다도 인코딩된 부호가 갖는 성분 수 Dy는 자연히 더 작을 거라고 생각하는데 항상 그렇지만은 않아요.
    * 즉, 희소 규제화를 이용한다면 여분의 자유도를 갖는 특징이어도 입력 데이터를 잘 나타내는 자질을 얻는 것이 가능해지는데, 이것을 **과완비(overcomplete)** 한 표현이라고 합니다.
* Q. 그럼 **희소 오토인코더(sparse autoencoder)** 는요?
    * 예를 들어서 중간층에 활성화 함수로 선형함수를 사용한 경우, 중간층의 유닛수 Dy가 입력층의 유닛 수 Dx보다 많다면 무의미한 결과밖에 얻을 수 없습니다. 활성화 함수에 비선형함수를 사용하면 이러한 논의는 처음부터 성립할 수 없지만, 중간층의 자유도가 입력의 자유도보다 크다는 것은 변하지 않으며, 쓸모없는 해만을 얻게 될 가능성이 높습니다. 이에 대해 희소 규제화를 사용하면, 증간층의 유닛 수가 더 많은 경우(Dy >= Dx)에도 오토인코더가 의미 있는 표현을 학습할 수 있게 되는데 이를 **희소 오토인코더**라고 부릅니다.

* Q. **희소규제화**에 대해서 좀 더 자세히 설명해주세요.
    * 기본적인 아이디어는 훈련 샘플 Xn을 되도록 적은 수의 중간층 유닛을 사용하여 재현할 수 있도록 파라미터를 결정하는 것입니다.
    * 또한, 입력 x로부터 중간층의 출력 y를 거쳐 출력 x'가 계산되는 과정에 있어서, y의 각 유닛 중 되도록 적은 수의 유닛만이 0이 아닌 출력치를 갖고 나머지는 출력이 0이 되도록(=발화하지 않음)하는 제약을 가합니다.
        * 원래의 오차함수 E(w)에 희소 규제화 항을 추가한 E'(w)를 최소화한다.
    * 간단히는 활성도의 평균값이 작아지도록 제약을 가하고 각 샘픔을 표현하는 데 쓰이는 중간층 유닛의 수가 적어지도록 학습을 진행한다고 설명할 수 있습니다.
* Q. 희소규제화에서 **로(ρ)** 와 **베타(β)** 는 무엇을 의미할까요?
    * 우선 로(ρ) 햇은 중간층의 유닛의 평균 활성도의 추정치를 나타내고 로(ρ)는 그 목표치가 되는 파라미터이다.
    * 원래의 오차함수 E(w)에 최소 규제화 항을 추가한 E'(W)를 최소화하면 중간층의 각 유닛 평균 활성도가 작은 ρ에 가깝게 되고, 입력의 재현오차 E(w)가 작아지도록 w가 정해진다.
    * 베타(β)는 이 두가지 목표의 밸런스를 결정하는 파라미터다.
* Q. 그렇다면 희소규제화 항에서의 **베타(β)** 에 따라 어떤 특징이 나타날까요?
    * MNIST를 예로 들면 β가 0, 즉 희소 규제화를 하지 않은 경우에 학습된 특징은 어수선한 패턴을 보입니다.
    * 이에 반해 β = 3.0 정도의 강한 규제화를 한다면 각각의 숫자가 그대로 특징으로 선택되고 맙니다.
    * 즉, 학습 시의 희소 규제화는 오토인코더의 중간층의 각 유닛을 '분담'하여 표현하는 양상을 제어하는 역할을 한다고 할 수 있습니다.
    * 희소 규제화를 하지 않을 때는 중간층의 유닛은 각각 제멋대로 입력을 표현하려고 하지만, 알맞은 정도로 희소 규제화가 행해지면, 입력이 갖는 구조를 효율적으로 표현할 수 있도록
    중간층의 유닛이 협력하여 각각의 입력을 표현하게 됩니다. 또, 희소 규제화가 너무 강하면 중간층 유닛이 집합으로서가 아닌, 되도록 단독으로 각각의 입력을 표현하려고 하는 경향이 있습니다.
* Q. **희소규제화**와 **가중치 감쇠**의 차이점은?
    * 가중치 감쇠의 규제화 항은 가중치 파라미터 그 자체에 대한 함수이므로 가중치의 업데이트 식만 수정하면 된다.
    * 하지만, 희소 규제화 항의 경우는 **해당 층 유닛의 활성도에 대한 제약**이기 때문에, 단지 그 충뿐 아니라 해당 층 아래에 있는 모든 층의 파라미터에 의해 정해지게 된다.
    * 따라서 일반적으로는 델타의 역전파 자체를 수정해야 한다는 차이점이 있다.
* Q. 배치 최적화와 미니배치에서 평균 활성도를 어떻게 구할까요?
    * 배치 최적화에서는 모든 샘플의 feed-forward 계산을 한 번 해서 각 유닛의 출력을 구한 뒤 활성도의 평균을 계산해야 합니다.
    * 하지만 미니배치를 사용하여 학습 중이라면, 평균 활성도만을 계산하기 위해 모든 샘플을 계산하는 것이 비효율적이므로 미니배치 내의 모든 샘플에 대해서만 평균 활성도를 구하는 것을 반복합니다.
* Q. **데이터의 백색화(whitneing)** 가 무엇입니까?
    * Trainning 데이터의 불필요한 경향은 학습을 방해하는 원인이 되는데, 학습 전 이를 처리하는 것이 **백색화(whitening)** 라고 합니다. (정규화와 비슷하지만 수준이 더 높음)
    * 백색화는 오토인코더가 좋은 자질을 학습할 수 있을지를 크게 좌우할 수 있습니다.
    * Q-1. 그럼 백색화를 왜 할까요?
        * 훈련 샘플에서 성분 간의 상관성을 제거하려고 백색화를 합니다.
    * Q-2. 정규화랑은 무슨 차이일까요?
        * 정규화는 단위 처리였던 데에 비해, 백색화는 성분 간의 관계를 수정하는 처리입니다.
* Q. **PCA 백색화**와 **ZCA 백색화**는 무엇인가요?
    * 공분산행렬의 고유벡터를 이용하는 것이 샘플 집합에 대한 주성분 분석(PCA)과 일맥상통한다는 점에서 P를 PCA 백색화라고 해요.
    * (P^T)P= Φ를 만족하는 P를 대칭행렬(P=P^T)로 제한하는 방법이 ZCA 백색화라 합니다.
        * 그리고 어떤 백색화를 사용하는 경우에도 데이터에 따라 특정 성분의 분산이 매우 작거나 극단적으로 0이 되는 경우가 있어서 작은 값(ε)을 사용합니다.
    * Q-1. PCA 백색화와 ZCA 백색화의 차이점은?
        * ZCA 백색화를 거친 샘플은 직류성분이 제거되어 이미지의 모서리가 강조되어 있는데 반해,
        PCA 백색화를 거친 샘플은 고주파성분이 이미지 전체로 강조되어 원래 이미지의 공간구조가 거의 남지 않은 것처럼 보여져요.
